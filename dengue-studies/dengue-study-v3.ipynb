{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Intro\n\nContinuing my work from Dengue Study V1 and V2. In this notebook, I will refine the models. Some ideas to implement:\n\n* Weighted loss functions: apply more weight to the beginning of the forecast (Done!)\n  * Previous results could got even get the day 1 forecast reasonably, which is surprising, because it should be very close to the previous day\n  * Could applying weighting to the loss function, with the day one forecast mattering more, help?\n* Better displays of the results (New plot looking at n-week forecast)\n* Early stopping\n\n\nSome more things to add:\n* Some measures of feature importance\n* Add the time of year as a feature? That may be complicated by the issues in the data set. Could get it from week_start_date, map it to day of year?\n\n","metadata":{}},{"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-13T02:04:02.580175Z","iopub.execute_input":"2023-09-13T02:04:02.580777Z","iopub.status.idle":"2023-09-13T02:04:03.794365Z","shell.execute_reply.started":"2023-09-13T02:04:02.580730Z","shell.execute_reply":"2023-09-13T02:04:03.793320Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Some stuff from the Feature Engineering Course:\nfrom sklearn.feature_selection import mutual_info_regression\n\ndef make_mi_scores(X, y, discrete_features):\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores","metadata":{"execution":{"iopub.status.busy":"2023-09-13T02:05:24.468196Z","iopub.execute_input":"2023-09-13T02:05:24.468875Z","iopub.status.idle":"2023-09-13T02:05:24.782280Z","shell.execute_reply.started":"2023-09-13T02:05:24.468830Z","shell.execute_reply":"2023-09-13T02:05:24.781199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some relevant info from google: female mosquito lifetime is 42-56 days, male is 10\nThat is approximately 7 weeks for female mosquitos.\n\n","metadata":{}},{"cell_type":"code","source":"dengue_features_train = pd.read_csv('../input/epidemy/dengue_features_train.csv',parse_dates=True,infer_datetime_format=True)\ndengue_features_test = pd.read_csv('../input/epidemy/dengue_features_test.csv',parse_dates=True,infer_datetime_format=True)\ndengue_labels_train   = pd.read_csv('../input/epidemy/dengue_labels_train.csv',parse_dates=True,infer_datetime_format=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-13T02:05:27.510009Z","iopub.execute_input":"2023-09-13T02:05:27.510610Z","iopub.status.idle":"2023-09-13T02:05:27.561996Z","shell.execute_reply.started":"2023-09-13T02:05:27.510554Z","shell.execute_reply":"2023-09-13T02:05:27.560980Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in dengue_features_train.columns:\n    print(col)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T02:05:29.784504Z","iopub.execute_input":"2023-09-13T02:05:29.784954Z","iopub.status.idle":"2023-09-13T02:05:29.793190Z","shell.execute_reply.started":"2023-09-13T02:05:29.784910Z","shell.execute_reply":"2023-09-13T02:05:29.791867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datetime import date, timedelta\nimport datetime\nimport math\n\ndebug_date_map=True\n\n# store information on \ndate_map_debug_logs=[]\n\nset_of_bad_years=set()\n\n# map from dataframe with year, weekofyear features\n# to a time index\n# lets go with weeks since day 1 of week 1 of 1990\ndef date_start_map(x):\n    year_one=1990\n    week_one=1\n    day_of_week=1\n    # Trying thursday\n    date_one = datetime.datetime.strptime(f\"{year_one}-U{week_one}-{day_of_week}\",\"%Y-U%U-%w\").date()\n\n    year_current = x['year']\n    week_current = x['weekofyear']\n\n\n    # trouble years are those where this data set incorrectly assigns a week at the \n    # beginning to the end. I've identified these as those starting with Friday, Saturday,\n    # or Sunday\n    raw_year_current = year_current\n    raw_week_current = week_current\n    trouble_year=False\n    shift = 0\n    year_start_date = datetime.datetime.strptime(f\"{year_current}-M{1}-{1}\",\"%Y-M%m-%d\").date()\n\n    date_two = datetime.datetime.strptime(f\"{year_current}-U{week_current}-{day_of_week}\",\"%Y-U%U-%w\").date()\n    if (year_start_date.weekday() >= 4):\n        trouble_year=True\n        set_of_bad_years.add(year_current)\n        # need better check\n        if (week_current >= 52):\n            year_current-=1\n            # trying to assign to Dec 31\n            date_two = datetime.datetime.strptime(f\"{year_current}-M{12}-{31}\",\"%Y-M%m-%d\").date()\n        else:\n            shift = 1\n            date_two = datetime.datetime.strptime(f\"{year_current}-U{week_current}-{day_of_week}\",\"%Y-U%U-%w\").date()    \n    time_d = date_two - date_one\n    weeks = math.ceil(time_d.days / 7.0)\n    weeks += shift\n    if debug_date_map:\n        debug_log=f\"{raw_year_current}-{raw_week_current} mapped to week {weeks} using days {time_d.days}\"\n        date_map_debug_logs.append(debug_log)\n    \n    return weeks\nstartmap=date_start_map\n\ndengue_features_train['weeks_since_start']=dengue_features_train.apply(startmap,axis=1)\ndengue_features_test['weeks_since_start']=dengue_features_test.apply(startmap,axis=1)\ndengue_labels_train['weeks_since_start']=dengue_labels_train.apply(startmap,axis=1)\n\ndengue_features_sj_train=pd.DataFrame(dengue_features_train[dengue_labels_train['city']=='sj'])\ndengue_features_sj_test=pd.DataFrame(dengue_features_test[dengue_labels_train['city']=='sj'])\ndengue_labels_sj_train=pd.DataFrame(dengue_labels_train[dengue_labels_train['city']=='sj'])\n\ndengue_features_iq_train=pd.DataFrame(dengue_features_train[dengue_labels_train['city']=='iq'])\ndengue_features_iq_test=pd.DataFrame(dengue_features_test[dengue_labels_train['city']=='iq'])\ndengue_labels_iq_train=pd.DataFrame(dengue_labels_train[dengue_labels_train['city']=='iq'])\n\nlist_of_frames=[\n    dengue_features_sj_train,\n    dengue_features_sj_test,\n    dengue_labels_sj_train,\n    dengue_features_iq_train,\n    dengue_features_iq_test,\n    dengue_labels_iq_train\n]\nfor frame in list_of_frames:\n    frame.sort_values('weeks_since_start',inplace=True)\n    frame.set_index('weeks_since_start',inplace=True)\n\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2023-09-13T02:05:32.654815Z","iopub.execute_input":"2023-09-13T02:05:32.655607Z","iopub.status.idle":"2023-09-13T02:05:33.079106Z","shell.execute_reply.started":"2023-09-13T02:05:32.655550Z","shell.execute_reply":"2023-09-13T02:05:33.077987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax=dengue_labels_sj_train.loc[:,['total_cases']].plot(figsize=(15,5))\ndengue_labels_iq_train.loc[:,['total_cases']].plot(ax=ax)\nax.legend(['San Juan','Iquitos'])\nax.set(xlabel=\"Week\",ylabel=\"Cases per Week\")\nplt.savefig('cases_data.png')","metadata":{"execution":{"iopub.status.busy":"2023-09-13T02:05:44.170960Z","iopub.execute_input":"2023-09-13T02:05:44.171944Z","iopub.status.idle":"2023-09-13T02:05:44.629434Z","shell.execute_reply.started":"2023-09-13T02:05:44.171890Z","shell.execute_reply":"2023-09-13T02:05:44.628310Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_week=50\nend_week=500\n\nfeatures_to_plot=['total_cases',\n                  'reanalysis_relative_humidity_percent',\n                  'reanalysis_precip_amt_kg_per_m2',\n                  'reanalysis_dew_point_temp_k',\n                  'reanalysis_specific_humidity_g_per_kg',\n                  'reanalysis_tdtr_k',\n                 'station_min_temp_c']\nfig, axes = plt.subplots(nrows=len(features_to_plot), ncols=1, sharex=True, sharey=False)\nfor i in range(len(features_to_plot)):\n    feat_to_plot = features_to_plot[i]\n    if feat_to_plot == 'total_cases':\n        figsize=(9,5)\n        ax1 = plt.subplot2grid((len(features_to_plot), 1), (0, 0), colspan=1)\n        dengue_labels_sj_train.loc[start_week:end_week,feat_to_plot].plot(ax=ax1,figsize=figsize)\n    else:\n        ax2 = plt.subplot2grid((len(features_to_plot), 1), (i, 0), colspan=1)\n        dengue_features_sj_train.loc[start_week:end_week,feat_to_plot].plot(ax=ax2,label=feat_to_plot)\n        ax2.legend()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-13T02:05:51.499106Z","iopub.execute_input":"2023-09-13T02:05:51.499860Z","iopub.status.idle":"2023-09-13T02:05:52.716874Z","shell.execute_reply.started":"2023-09-13T02:05:51.499817Z","shell.execute_reply":"2023-09-13T02:05:52.715869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from learntools.time_series.utils import plot_lags, make_lags, make_leads\nfrom statsmodels.graphics.tsaplots import plot_pacf\n\nplot_pacf(dengue_labels_sj_train['total_cases'], lags=14);\nplt.savefig('cases_sj_pacf.png')","metadata":{"execution":{"iopub.status.busy":"2023-09-13T02:06:05.878772Z","iopub.execute_input":"2023-09-13T02:06:05.879192Z","iopub.status.idle":"2023-09-13T02:06:06.494445Z","shell.execute_reply.started":"2023-09-13T02:06:05.879155Z","shell.execute_reply":"2023-09-13T02:06:06.493297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Variables\n","metadata":{}},{"cell_type":"code","source":"# Averaging Vegetation Index data\nndvi_features=['ndvi_se','ndvi_sw','ndvi_ne','ndvi_nw']\n\ndengue_features_sj_train_ndvi_mean=dengue_features_sj_train[ndvi_features].mean(axis=1)\ndengue_features_sj_train['ndvi_mean']=dengue_features_sj_train_ndvi_mean","metadata":{"execution":{"iopub.status.busy":"2023-09-13T02:06:10.982665Z","iopub.execute_input":"2023-09-13T02:06:10.983380Z","iopub.status.idle":"2023-09-13T02:06:10.993979Z","shell.execute_reply.started":"2023-09-13T02:06:10.983339Z","shell.execute_reply":"2023-09-13T02:06:10.992794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"precip_variable='reanalysis_sat_precip_amt_mm'\nndvi_variable='ndvi_mean'\nrel_hum_variable='reanalysis_relative_humidity_percent'\ndew_point_variable='reanalysis_dew_point_temp_k'\nmean_temp_variable='reanalysis_avg_temp_k'\n\nvariables_of_interest=[precip_variable,ndvi_variable,\n                       rel_hum_variable,dew_point_variable,\n                       mean_temp_variable]\n# for labelling\nvarnames_of_interest=['precip','ndvi','rel_hum','dew_pt','mean_temp']\nvartitles_of_interest=['Precipitation','Vegetation Index','Relative Humidity','Dew Point','Mean Temp']","metadata":{"execution":{"iopub.status.busy":"2023-09-13T02:06:14.322684Z","iopub.execute_input":"2023-09-13T02:06:14.323412Z","iopub.status.idle":"2023-09-13T02:06:14.330036Z","shell.execute_reply.started":"2023-09-13T02:06:14.323371Z","shell.execute_reply":"2023-09-13T02:06:14.328873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"smoothed_features=[]\nfor i in range(len(variables_of_interest)):\n    variable=variables_of_interest[i]\n    varname=varnames_of_interest[i]\n    vartitle=vartitles_of_interest[i]\n    variable_rolling = dengue_features_sj_train[variable].rolling(window=5,center=False).mean()\n    smoothed_features.append(variable_rolling)\n    ax = dengue_features_sj_train[variable].plot(label=vartitle)\n    variable_rolling.plot(ax=ax,label=\"%s (smoothed)\" % (vartitle))\n    ax.legend()\n    #ax.set_ylabel(vartitle_of_interest[i])\n    plt.savefig(\"variable_%s_smoothed.png\" % (varname))\n    ax.clear()","metadata":{"execution":{"iopub.status.busy":"2023-09-13T02:06:17.046312Z","iopub.execute_input":"2023-09-13T02:06:17.047282Z","iopub.status.idle":"2023-09-13T02:06:18.221773Z","shell.execute_reply.started":"2023-09-13T02:06:17.047223Z","shell.execute_reply":"2023-09-13T02:06:18.220589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Section: Target and Feature Lags","metadata":{}},{"cell_type":"code","source":"y_sj=dengue_labels_sj_train['total_cases']","metadata":{"execution":{"iopub.status.busy":"2023-09-13T02:07:05.721846Z","iopub.execute_input":"2023-09-13T02:07:05.722270Z","iopub.status.idle":"2023-09-13T02:07:05.727460Z","shell.execute_reply.started":"2023-09-13T02:07:05.722230Z","shell.execute_reply":"2023-09-13T02:07:05.726363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_lag_number = 12\n\ny_sj_lags=make_lags(dengue_labels_sj_train['total_cases'],lags=target_lag_number)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T02:09:11.402082Z","iopub.execute_input":"2023-09-13T02:09:11.403069Z","iopub.status.idle":"2023-09-13T02:09:11.414359Z","shell.execute_reply.started":"2023-09-13T02:09:11.403001Z","shell.execute_reply":"2023-09-13T02:09:11.413202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# can use code from Time Series course\n\nn_lags=24\n\nlags_list=[]\nlags_list_no_target=[]\nfor i in range(len(variables_of_interest)):\n    var_lags=make_lags(dengue_features_sj_train[variables_of_interest[i]],\n                       lags=n_lags,name=varnames_of_interest[i])\n    lags_list.append(var_lags)\n    lags_list_no_target.append(var_lags)\n    \n# add target lags\nlags_list.append(y_sj_lags)\n    \nsmoothed_lags_list=[]\nsmoothed_lags_list_no_target=[]\nfor i in range(len(variables_of_interest)):\n    var_lags=make_lags(smoothed_features[i],\n                       lags=n_lags,name=\"%s_smooth\" % (varnames_of_interest[i]))\n    print(type(var_lags))\n    smoothed_lags_list.append(var_lags)\n    smoothed_lags_list_no_target.append(var_lags)\n\nsmoothed_lags_list.append(y_sj_lags)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-13T02:09:31.270612Z","iopub.execute_input":"2023-09-13T02:09:31.270995Z","iopub.status.idle":"2023-09-13T02:09:31.330497Z","shell.execute_reply.started":"2023-09-13T02:09:31.270959Z","shell.execute_reply":"2023-09-13T02:09:31.329294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_sj_deriv=dengue_labels_sj_train['total_cases']-y_sj_lags['y_lag_1']","metadata":{"execution":{"iopub.status.busy":"2023-09-13T02:09:34.800812Z","iopub.execute_input":"2023-09-13T02:09:34.801569Z","iopub.status.idle":"2023-09-13T02:09:34.807645Z","shell.execute_reply.started":"2023-09-13T02:09:34.801520Z","shell.execute_reply":"2023-09-13T02:09:34.806542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_sj_deriv_smooth_gauss_mean=y_sj_deriv.rolling(window=7,center=False,win_type='gaussian').mean(std=3)\ny_sj_deriv_smooth_const_mean=y_sj_deriv.rolling(window=7,center=False).mean(std=3)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-13T02:09:36.716473Z","iopub.execute_input":"2023-09-13T02:09:36.716879Z","iopub.status.idle":"2023-09-13T02:09:36.725496Z","shell.execute_reply.started":"2023-09-13T02:09:36.716841Z","shell.execute_reply":"2023-09-13T02:09:36.724169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"START=50\nEND=500\nax=y_sj_deriv.loc[START:END].plot(figsize=(15,5),color='black',alpha=0.3)\ny_sj_deriv_smooth_gauss_mean.loc[START:END].plot(ax=ax,label='Gaussian Smooth',color='red')\ny_sj_deriv_smooth_const_mean.loc[START:END].plot(ax=ax,label='Flat Window',color='purple')\nax.set(xlabel=\"Week\",ylabel=\"Change in Cases per Week\")\nplt.savefig(\"deriv_smoothing.png\")","metadata":{"execution":{"iopub.status.busy":"2023-09-13T02:09:41.262594Z","iopub.execute_input":"2023-09-13T02:09:41.262974Z","iopub.status.idle":"2023-09-13T02:09:41.640954Z","shell.execute_reply.started":"2023-09-13T02:09:41.262938Z","shell.execute_reply":"2023-09-13T02:09:41.639964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ny_sj_deriv_smooth_gauss_mean_frame=pd.DataFrame(y_sj_deriv_smooth_gauss_mean,columns=['target_deriv_smooth_gauss_mean'])\ny_sj_deriv_smooth_const_mean_frame=pd.DataFrame(y_sj_deriv_smooth_const_mean,columns=['target_deriv_smooth_const_mean'])\n\nprint(y_sj_deriv_smooth_gauss_mean_frame.head(15))\n# need to shift by one to make sure it doesn't include the current or future information\n# Or, apply make lags on this.\n","metadata":{"execution":{"iopub.status.busy":"2023-09-13T02:09:47.622774Z","iopub.execute_input":"2023-09-13T02:09:47.623559Z","iopub.status.idle":"2023-09-13T02:09:47.635961Z","shell.execute_reply.started":"2023-09-13T02:09:47.623516Z","shell.execute_reply":"2023-09-13T02:09:47.634697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_deriv_smooth_gauss_lags=make_lags(y_sj_deriv_smooth_gauss_mean_frame['target_deriv_smooth_gauss_mean'],\n                                         lags=n_lags,name='target_deriv_smooth_gauss_mean')\ntarget_deriv_smooth_const_lags=make_lags(y_sj_deriv_smooth_const_mean_frame['target_deriv_smooth_const_mean'],\n                                         lags=n_lags,name='target_deriv_smooth_const_mean')\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-13T02:10:06.026128Z","iopub.execute_input":"2023-09-13T02:10:06.027088Z","iopub.status.idle":"2023-09-13T02:10:06.046999Z","shell.execute_reply.started":"2023-09-13T02:10:06.027009Z","shell.execute_reply":"2023-09-13T02:10:06.045841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(type(target_deriv_smooth_const_lags))\nprint(type(target_deriv_smooth_gauss_lags))","metadata":{"execution":{"iopub.status.busy":"2023-09-13T02:10:09.696719Z","iopub.execute_input":"2023-09-13T02:10:09.697131Z","iopub.status.idle":"2023-09-13T02:10:09.703640Z","shell.execute_reply.started":"2023-09-13T02:10:09.697091Z","shell.execute_reply":"2023-09-13T02:10:09.702328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lags_list\n\n# Adding the derivatives to the lag lists\nlags_list.append(target_deriv_smooth_const_lags)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-13T02:10:12.104401Z","iopub.execute_input":"2023-09-13T02:10:12.105446Z","iopub.status.idle":"2023-09-13T02:10:12.111166Z","shell.execute_reply.started":"2023-09-13T02:10:12.105394Z","shell.execute_reply":"2023-09-13T02:10:12.110058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for lag_feature in lags_list:\n    print(type(lag_feature))\n    for col in lag_feature.columns:\n        print(col)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T02:10:14.636177Z","iopub.execute_input":"2023-09-13T02:10:14.637203Z","iopub.status.idle":"2023-09-13T02:10:14.644627Z","shell.execute_reply.started":"2023-09-13T02:10:14.637162Z","shell.execute_reply":"2023-09-13T02:10:14.643520Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dengue_features_sj_train_lags=pd.concat(lags_list, axis=1)\ndengue_features_no_target_sj_train_lags=pd.concat(lags_list_no_target, axis=1)\n\ndengue_smoothed_features_sj_train_lags=pd.concat(smoothed_lags_list, axis=1)\ndengue_smoothed_features_no_target_sj_train_lags=pd.concat(smoothed_lags_list_no_target, axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T02:10:22.442127Z","iopub.execute_input":"2023-09-13T02:10:22.448807Z","iopub.status.idle":"2023-09-13T02:10:22.490475Z","shell.execute_reply.started":"2023-09-13T02:10:22.448757Z","shell.execute_reply":"2023-09-13T02:10:22.485177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nX_lags_no_target_for_mi=dengue_features_no_target_sj_train_lags.dropna()\nX_lags_for_mi=dengue_features_sj_train_lags.dropna()\n#print(X_lags_no_target_for_mi)\n\n(y_lags_no_target_for_mi,X_lags_no_target_for_mi) = y_sj.align(X_lags_no_target_for_mi, join='inner', axis=0)\n(y_lags_for_mi,X_lags_for_mi) = y_sj.align(X_lags_for_mi, join='inner', axis=0)\nprint(X_lags_no_target_for_mi.shape)\nprint(y_lags_no_target_for_mi.shape)\n\nmi_scores = make_mi_scores(X_lags_no_target_for_mi, y_lags_no_target_for_mi, 'auto')\nmi_with_target_scores = make_mi_scores(X_lags_for_mi, y_lags_for_mi, 'auto')","metadata":{"execution":{"iopub.status.busy":"2023-09-13T02:10:25.422446Z","iopub.execute_input":"2023-09-13T02:10:25.423652Z","iopub.status.idle":"2023-09-13T02:10:26.690318Z","shell.execute_reply.started":"2023-09-13T02:10:25.423605Z","shell.execute_reply":"2023-09-13T02:10:26.689110Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Mutual Information Scores without target info:\")\nprint(mi_scores.head(24))\nprint(\"Mutual Information Scores, including target-derived features\")\nprint(mi_with_target_scores.head(25))","metadata":{"execution":{"iopub.status.busy":"2023-09-13T02:10:30.152808Z","iopub.execute_input":"2023-09-13T02:10:30.153285Z","iopub.status.idle":"2023-09-13T02:10:30.172666Z","shell.execute_reply.started":"2023-09-13T02:10:30.153238Z","shell.execute_reply":"2023-09-13T02:10:30.171526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot MI scores for y_lag_n\n# target_lag_number\ny_lag_n = []\ny_lag_mi_scores = []\nfor i in range(1,target_lag_number+1):\n    y_lag_n.append(i)\n    y_lag_mi_scores.append(mi_with_target_scores[f'y_lag_{i}'])\n    \nfig,ax = plt.subplots()\nplt.plot(y_lag_n,y_lag_mi_scores,label=\"Mutual Information Score for lag n\")\nplt.legend()\nax.set_ylabel('Mutual Information Score')\nax.set_xlabel('Week Lag n')\n","metadata":{"execution":{"iopub.status.busy":"2023-09-13T02:10:35.248589Z","iopub.execute_input":"2023-09-13T02:10:35.249224Z","iopub.status.idle":"2023-09-13T02:10:35.497961Z","shell.execute_reply.started":"2023-09-13T02:10:35.249182Z","shell.execute_reply":"2023-09-13T02:10:35.496931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndef make_multistep_target(ts, steps):\n    return pd.concat(\n        {f'y_step_{i + 1}': ts.shift(-i)\n         for i in range(steps)},\n        axis=1)\n\ndef plot_multistep(y, every=1, ax=None, palette_kwargs=None):\n    palette_kwargs_ = dict(palette='husl', n_colors=6, desat=None)\n    #palette_kwargs_ = dict(palette='husl', n_colors=16, desat=None)\n    if palette_kwargs is not None:\n        palette_kwargs_.update(palette_kwargs)\n    palette = sns.color_palette(**palette_kwargs_)\n    if ax is None:\n        fig, ax = plt.subplots()\n    ax.set_prop_cycle(plt.cycler('color', palette))\n    for date, preds in y[::every].iterrows():\n        #preds.index = pd.period_range(start=date, periods=len(preds))\n        preds.index = range(date,date+len(preds))\n        preds.plot(ax=ax,label=f\"Forecast from {date}\")\n    return ax\n","metadata":{"execution":{"iopub.status.busy":"2023-09-13T02:10:51.372612Z","iopub.execute_input":"2023-09-13T02:10:51.373001Z","iopub.status.idle":"2023-09-13T02:10:51.383818Z","shell.execute_reply.started":"2023-09-13T02:10:51.372967Z","shell.execute_reply":"2023-09-13T02:10:51.382646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot the nstep step\n# i.e. plot the forecast for nsteps ahead\ndef plot_nth_step(y, nstep=1,ax=None,palette_kwargs=None):\n    #palette_kwargs_ = dict(palette='husl', n_colors=6, desat=None)\n    #if palette_kwargs is not None:\n    #    palette_kwargs_.update(palette_kwargs)\n    #palette = sns.color_palette(**palette_kwargs_)\n    if ax is None:\n        fig, ax = plt.subplots()\n    #ax.set_prop_cycle(plt.cycler('color', palette))\n    \n    # might be shift(-nstep)\n    preds=y.iloc[:,nstep].shift(nstep)\n    preds.plot(ax=ax,label=f\"Forecast from {nstep} weeks earlier\")\n    return ax","metadata":{"execution":{"iopub.status.busy":"2023-09-13T02:11:08.366476Z","iopub.execute_input":"2023-09-13T02:11:08.366877Z","iopub.status.idle":"2023-09-13T02:11:08.374074Z","shell.execute_reply.started":"2023-09-13T02:11:08.366840Z","shell.execute_reply":"2023-09-13T02:11:08.372780Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_forecast_steps=20\ny_sj_multistep = make_multistep_target(y_sj,steps=target_forecast_steps).dropna()","metadata":{"execution":{"iopub.status.busy":"2023-09-13T02:11:11.645640Z","iopub.execute_input":"2023-09-13T02:11:11.646047Z","iopub.status.idle":"2023-09-13T02:11:11.660944Z","shell.execute_reply.started":"2023-09-13T02:11:11.645993Z","shell.execute_reply":"2023-09-13T02:11:11.659805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining Inputs and Models","metadata":{}},{"cell_type":"code","source":"# each model may require different X,y\n# especially based on rows we remove\n\nlist_X=[]\nlist_y=[]\n\nmodel_list=[]\nmodel_labels=[]\nmodel_titles=[]\n\nX_train_list=[]\nX_valid_list=[]\ny_train_list=[]\ny_valid_list=[]","metadata":{"execution":{"iopub.status.busy":"2023-09-13T02:11:19.554126Z","iopub.execute_input":"2023-09-13T02:11:19.555195Z","iopub.status.idle":"2023-09-13T02:11:19.561761Z","shell.execute_reply.started":"2023-09-13T02:11:19.555152Z","shell.execute_reply":"2023-09-13T02:11:19.560401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining Models","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import Ridge,Lasso\nfrom sklearn.linear_model import LinearRegression\nfrom xgboost import XGBRegressor\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.multioutput import RegressorChain\nfrom sklearn.metrics import mean_squared_error","metadata":{"execution":{"iopub.status.busy":"2023-09-13T02:11:22.070828Z","iopub.execute_input":"2023-09-13T02:11:22.071234Z","iopub.status.idle":"2023-09-13T02:11:22.166995Z","shell.execute_reply.started":"2023-09-13T02:11:22.071195Z","shell.execute_reply":"2023-09-13T02:11:22.165791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Available columns:\")\nfor col in dengue_features_sj_train_lags.columns:\n    print(col)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T02:11:32.790284Z","iopub.execute_input":"2023-09-13T02:11:32.790835Z","iopub.status.idle":"2023-09-13T02:11:32.803359Z","shell.execute_reply.started":"2023-09-13T02:11:32.790792Z","shell.execute_reply":"2023-09-13T02:11:32.801794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_X=[]\nlist_y=[]\n\nmodel_list=[]\nmodel_labels=[]\nmodel_titles=[]\n\nX_train_list=[]\nX_valid_list=[]\ny_train_list=[]\ny_valid_list=[]\n\n\n\nn_estimators=30\nn_jobs=4\n\nmodel_labels.append(\"RegChain1_Target_Only\")\nmodel_titles.append(\"Regressor Chain 1 Target Only\")\nmodel_1 = RegressorChain(XGBRegressor(n_estimators=n_estimators,n_jobs=n_jobs))\nmodel_list.append(model_1)\n\nfeatures_for_model=['y_lag_1','y_lag_2','y_lag_3','y_lag_4','y_lag_5',\n                    'y_lag_6','y_lag_7','y_lag_8','y_lag_9','y_lag_10'\n                   ]\n\nlocal_X=dengue_features_sj_train_lags[features_for_model].dropna()\nlocal_y=y_sj_multistep\nlist_X.append(local_X)\nlist_y.append(local_y)\n\n\nmodel_labels.append(\"RegChain1_Target_Only_More_Lags\")\nmodel_titles.append(\"Regressor Chain 1 Target Only More Lags\")\nmodel_1 = RegressorChain(XGBRegressor(n_estimators=n_estimators,n_jobs=n_jobs))\nmodel_list.append(model_1)\n\n\nfeatures_for_model=[]\nfor col in dengue_features_sj_train_lags.columns:\n    if 'y_lag' in col:\n        features_for_model.append(col)\n\n\nlocal_X=dengue_features_sj_train_lags[features_for_model].dropna()\nlocal_y=y_sj_multistep\nlist_X.append(local_X)\nlist_y.append(local_y)\n\n\n\n\n\n\n\n\n\n\nmodel_labels.append(\"RegChain1_Dew_Precip\")\nmodel_titles.append(\"Regressor Chain 1 Dew Precip\")\nmodel = RegressorChain(XGBRegressor(n_estimators=n_estimators,n_jobs=n_jobs))\nmodel_list.append(model)\nfeatures_for_model=['y_lag_1','y_lag_2','y_lag_3','y_lag_4','y_lag_5',\n                    'y_lag_6','y_lag_7','y_lag_8','y_lag_9','y_lag_10',\n                    'dew_pt_lag_1','dew_pt_lag_2','dew_pt_lag_3',\n                    'dew_pt_lag_4','dew_pt_lag_5','dew_pt_lag_6',\n                    'dew_pt_lag_7','dew_pt_lag_8','dew_pt_lag_9',\n                    'dew_pt_lag_10','dew_pt_lag_11','dew_pt_lag_12',\n                    'precip_lag_1','precip_lag_2','precip_lag_3',\n                    'precip_lag_4','precip_lag_5','precip_lag_6',\n                    'precip_lag_7','precip_lag_8','precip_lag_9',\n                    'precip_lag_10','precip_lag_11','precip_lag_12'\n                   ]\n\n\nlocal_X=dengue_features_sj_train_lags[features_for_model].dropna()\nlocal_y=y_sj_multistep\nlist_X.append(local_X)\nlist_y.append(local_y)\n\nmodel_labels.append(\"RegChain1_Dew_Temp_Hum\")\nmodel_titles.append(\"Regressor Chain 1 Dew Temp Hum\")\nmodel = RegressorChain(XGBRegressor(n_estimators=n_estimators,n_jobs=n_jobs))\nmodel_list.append(model)\n\n\nfeatures_for_model=['y_lag_1','y_lag_2','y_lag_3','y_lag_4','y_lag_5',\n                    'y_lag_6','y_lag_7','y_lag_8','y_lag_9','y_lag_10',\n                    'dew_pt_lag_6','dew_pt_lag_7','dew_pt_lag_8','dew_pt_lag_9',\n                    'dew_pt_lag_10','dew_pt_lag_11','dew_pt_lag_12',\n                    'mean_temp_lag_6','mean_temp_lag_7','mean_temp_lag_8',\n                    'mean_temp_lag_9','mean_temp_lag_10','mean_temp_lag_11',\n                    'rel_hum_lag_6','rel_hum_lag_7','rel_hum_lag_8',\n                    'rel_hum_lag_9','rel_hum_lag_10','rel_hum_lag_11'\n                   ]\n\nlocal_X=dengue_features_sj_train_lags[features_for_model].dropna()\nlocal_y=y_sj_multistep\nlist_X.append(local_X)\nlist_y.append(local_y)\n\n\nmodel_labels.append(\"RegChain1_Dew_Temp_Hum_Precip\")\nmodel_titles.append(\"Regressor Chain 1 Dew Temp Hum Precip\")\nmodel = RegressorChain(XGBRegressor(n_estimators=n_estimators,n_jobs=n_jobs))\nmodel_list.append(model)\n\nfeatures_for_model=['y_lag_1','y_lag_2','y_lag_3','y_lag_4','y_lag_5',\n                    'y_lag_6','y_lag_7','y_lag_8','y_lag_9','y_lag_10',\n                    'dew_pt_lag_6','dew_pt_lag_7','dew_pt_lag_8','dew_pt_lag_9',\n                    'dew_pt_lag_10','dew_pt_lag_11','dew_pt_lag_12',\n                    'mean_temp_lag_6','mean_temp_lag_7','mean_temp_lag_8',\n                    'mean_temp_lag_9','mean_temp_lag_10','mean_temp_lag_11',\n                    'rel_hum_lag_6','rel_hum_lag_7','rel_hum_lag_8',\n                    'rel_hum_lag_9','rel_hum_lag_10','rel_hum_lag_11',\n                    'precip_lag_2','precip_lag_3','precip_lag_4','precip_lag_5',\n                    'precip_lag_6','precip_lag_7','precip_lag_8','precip_lag_9'\n                   ]\n\nlocal_X=dengue_features_sj_train_lags[features_for_model].dropna()\nlocal_y=y_sj_multistep\nlist_X.append(local_X)\nlist_y.append(local_y)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T02:15:07.650867Z","iopub.execute_input":"2023-09-13T02:15:07.651397Z","iopub.status.idle":"2023-09-13T02:15:07.699224Z","shell.execute_reply.started":"2023-09-13T02:15:07.651346Z","shell.execute_reply":"2023-09-13T02:15:07.697999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Neural Network models\n\n# need to make multiple outputs.\n\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dense,BatchNormalization,Dropout\nfrom tensorflow.keras import Input, Model\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import EarlyStopping","metadata":{"execution":{"iopub.status.busy":"2023-09-13T02:15:12.950611Z","iopub.execute_input":"2023-09-13T02:15:12.951252Z","iopub.status.idle":"2023-09-13T02:15:20.867823Z","shell.execute_reply.started":"2023-09-13T02:15:12.951211Z","shell.execute_reply":"2023-09-13T02:15:20.866610Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# New: Define my NNs with a Class\n# getting this to work is still beyond me\nclass MyNN1(Model):\n    def __init__(self,input_shape, *args, **kwargs):\n        #super(MyNN1, self).__init__()\n        #my_init = keras.initializers.glorot_uniform(seed=1)\n        super().__init__(*args, **kwargs)\n        \n        #self.input_layer = Input(input_shape,name='input')\n        self.layer1 = BatchNormalization()\n        self.layer2 = Dense(20, activation='relu',name='layer2')\n        #self.layer3 = BatchNormalization()\n        self.layer3 = Dense(20, activation='relu',name='layer3')\n        self.layer4 = Dense(20, activation='relu',name='layer4')\n        self.layer5 = Dense(20, activation='sigmoid',name='layer5')\n        self.layer6 = Dense(20, activation=None,name='layer6')\n\n    \n    # Getting this error: ValueError: Input 0 of layer \"layer2\" is incompatible with the layer: \n    #.      expected min_ndim=2, found ndim=1. Full shape received: (22,)\n    \n    def call(self, inputs):\n        #x = self.input_layer(inputs)\n        x = self.layer1(inputs)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.layer5(x)\n        x = self.layer6(x)\n        return x\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-13T02:15:24.063912Z","iopub.execute_input":"2023-09-13T02:15:24.065105Z","iopub.status.idle":"2023-09-13T02:15:24.077715Z","shell.execute_reply.started":"2023-09-13T02:15:24.065058Z","shell.execute_reply":"2023-09-13T02:15:24.076571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def BuildMyNN(input_shape,n_outputs,name):\n    inputs = Input(shape=(local_X.shape[1],),name='input')\n    #print(inputs)\n\n    x = BatchNormalization()(inputs)\n    x = Dense(200, activation='relu',name='layer2')(x)\n    x = Dropout(0.4)(x)\n    x = Dense(200, activation='sigmoid',name='layer3')(x)\n    x = Dropout(0.4)(x)\n    x = Dense(200, activation='relu',name='layer4')(x)\n    #x = Dense(20, activation=None,name='layer5')(x)\n    # FIXME verifying that the 20 has to be set to the output shape\n    x = Dense(n_outputs, activation=None,name='output')(x)\n\n#    x = BatchNormalization()(inputs)\n#    x = Dense(20, activation='relu',name='layer2')(x)\n#    x = BatchNormalization()(x)\n#    x = Dense(20, activation='relu',name='layer3')(x)\n#    x = Dense(20, activation='sigmoid',name='layer4')(x)\n#    x = Dense(20, activation=None,name='layer5')(x)\n\n    \n    model = Model(inputs=inputs,outputs=x)\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-09-13T02:15:26.728938Z","iopub.execute_input":"2023-09-13T02:15:26.729690Z","iopub.status.idle":"2023-09-13T02:15:26.738690Z","shell.execute_reply.started":"2023-09-13T02:15:26.729648Z","shell.execute_reply":"2023-09-13T02:15:26.737320Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Custom Loss Function\n\nWant to weight more strongly the early (1-week, 2-week, etc) losses.\nA simple, natural function to do this would be an exponential decay. To make sure that there is still some weighting on the farther out forecasts, let's use a decay to a constant.\n\n$$\nf(x) = (1 + \\alpha e^{-\\lambda (x - 1)})\n$$\n\nThe hyperparameter $\\lambda$ gives the decay of this enhanced loss, in units of weeks. For every week after the 1-week losses, the added weight decreases by a factor of $e^{-\\lambda}$. The half-life of this decay is given by $\\tau_{1/2} = \\frac{\\ln(2)}{\\lambda} \\approx \\frac{0.693}{\\lambda}$ \n\nThe hyperparameter $\\alpha$ gives the strength of the enhancement for the 1-week forecast (and overall scaling for the enhancement).\n\nNote: in the implementation, there will be no -1 shift, as the first (index 0) output column is already the 1-week forecast.\n\nIt seems this could also be done with the loss_weights option in compile()","metadata":{}},{"cell_type":"code","source":"# New: Define Custom Loss Functions\n# guide https://towardsdatascience.com/how-to-create-a-custom-loss-function-keras-3a89156ec69b\n# https://stackoverflow.com/questions/45480820/keras-how-to-get-tensor-dimensions-inside-custom-loss\nimport tensorflow.keras.backend as K\n\ncustom_loss_alpha = 15\n\n# start with halflife for readability\ncustom_loss_halflife = 1\ncustom_loss_lambda = np.log(2) / custom_loss_halflife\n\n# mse with exponential decay\ndef custom_loss_mse_weighted(y_true, y_pred):\n    # y_true and y_pred should be matrices of (batch_size * outputs)\n    # compute square error\n    loss = K.square(y_pred - y_true)  # (batch_size, output_size)\n    \n    output_dim = K.int_shape(y_true)[1]\n    weights = np.ones(output_dim)\n    weights_enhancement = np.arange(output_dim)\n    weights_enhancement = -custom_loss_lambda * weights_enhancement\n    weights_enhancement = custom_loss_alpha * np.exp(weights_enhancement)\n    weights = weights + weights_enhancement\n    \n    # multiplying the values with weights along batch dimension\n    loss = loss * weights          # (batch_size, output_size)\n    \n    # summing all loss values along batch dimension \n    loss = K.sum(loss, axis=1)        # (batch_size,)\n    \n    return loss\n\n# log_mse with exponential decay\ndef custom_loss_msle_weighted(y_true, y_pred):\n    # y_true and y_pred should be matrices of (batch_size * outputs)\n    # compute square error\n    # FIXME switch to keras log function\n    # FIXME should a mean be taken somewhere? A mean avoiding nans?\n    # But do I want to ignore nans? I should be punishing them.\n    #   maybe fillnan with a bad value? But that would have zero derivative\n    \n    # FIXME do we need to take the absolute value in case y_pred goes negative?\n    # increasing from standard offset of 1 to better avoid log(0)\n    msle_offset = 10\n    \n    loss = K.square(K.log(msle_offset+y_pred) - K.log(msle_offset+y_true))  # (batch_size, output_size)\n    \n    output_dim = K.int_shape(y_true)[1]\n    weights = np.ones(output_dim)\n    weights_enhancement = np.arange(output_dim)\n    weights_enhancement = -custom_loss_lambda * weights_enhancement\n    weights_enhancement = custom_loss_alpha * np.exp(weights_enhancement)\n    weights = weights + weights_enhancement\n    \n    # multiplying the values with weights along batch dimension\n    loss = loss * weights          # (batch_size, output_size)\n    \n    # summing both loss values along batch dimension \n    loss = K.sum(loss, axis=1)        # (batch_size,)\n    \n    return loss\n","metadata":{"execution":{"iopub.status.busy":"2023-09-13T02:15:36.050262Z","iopub.execute_input":"2023-09-13T02:15:36.051265Z","iopub.status.idle":"2023-09-13T02:15:36.064957Z","shell.execute_reply.started":"2023-09-13T02:15:36.051207Z","shell.execute_reply":"2023-09-13T02:15:36.063536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# NN Model 1\n\n\n# temporarily resetting lists for testing\nif (False):\n    list_X=[]\n    list_y=[]\n\n    model_list=[]\n    model_labels=[]\n    model_titles=[]\n\n    X_train_list=[]\n    X_valid_list=[]\n    y_train_list=[]\n    y_valid_list=[]\n\n    \n#nn_loss_choice='mean_squared_logarithmic_error'\nnn_loss_choice='mse'\n\nnn_loss_choice_a = 'mse'\nnn_loss_choice_b = custom_loss_mse_weighted\nnn_loss_choice_c='mean_squared_logarithmic_error'\nnn_loss_choice_d = custom_loss_msle_weighted\n\n# would some loss options emphasize the peaked outbreaks better?\n#Should the different time step outputs have different losses?\n    \n# use a,b,c,d for loss functions\n    \n    \n# Dew\nmodel_labels.append(\"NN_1a_Dew_Auto\")\nmodel_titles.append(\"Neural Network 1a Dew Auto\")\n#features_for_model=['y_lag_1','y_lag_2','y_lag_3','y_lag_4','y_lag_5',\n#                    'y_lag_6','y_lag_7','y_lag_8','y_lag_9','y_lag_10',\n#                    'dew_pt_lag_1','dew_pt_lag_2','dew_pt_lag_3',\n#                    'dew_pt_lag_4','dew_pt_lag_5','dew_pt_lag_6',\n#                    'dew_pt_lag_7','dew_pt_lag_8','dew_pt_lag_9',\n#                    'dew_pt_lag_10','dew_pt_lag_11','dew_pt_lag_12'\n#                   ]\nfeatures_for_model=['dew_pt_lag_1','dew_pt_lag_2','dew_pt_lag_3',\n                    'dew_pt_lag_4','dew_pt_lag_5','dew_pt_lag_6',\n                    'dew_pt_lag_7','dew_pt_lag_8','dew_pt_lag_9',\n                    'dew_pt_lag_10','dew_pt_lag_11','dew_pt_lag_12'\n                   ]\n# adding in all y_lag \nfor col in dengue_features_sj_train_lags.columns:\n    if 'y_lag' in col:\n        features_for_model.append(col)\n\nlocal_X=dengue_features_sj_train_lags[features_for_model].dropna()\nlocal_y=y_sj_multistep\n\nprint(local_X.shape[1])\ninputs = Input(shape=(local_X.shape[1],),name='input')\nprint(inputs)\n#model = MyNN1((local_X.shape[1],),name = \"NN_1a_Dew_Auto\")\n#model.build(input_shape=(local_X.shape[1],))\n#model = Model(inputs=inputs,outputs=x)\n#model = Model(inputs=inputs,outputs=output_layers)\n\n\nmodel = BuildMyNN((local_X.shape[1],),target_forecast_steps,\"NN_1a_Dew_Auto\")\nmodel.compile(loss=nn_loss_choice_a,optimizer='adam')\n    \nprint(model)\nmodel.summary()\nkeras.utils.plot_model(model, \"model_nn1a.png\", show_shapes=True,show_layer_activations=False)\nmodel_list.append(model)\nlist_X.append(local_X)\nlist_y.append(local_y)\n\n\n\n\n# Now with weighted training\nmodel_labels.append(\"NN_1b_Dew_Auto\")\nmodel_titles.append(\"Neural Network 1b Dew Auto\")\n\nlocal_X=dengue_features_sj_train_lags[features_for_model].dropna()\nlocal_y=y_sj_multistep\n\ninputs = Input(shape=(local_X.shape[1],),name='input')\n\nmodel = BuildMyNN((local_X.shape[1],),target_forecast_steps,\"NN_1b_Dew_Auto\")\nmodel.compile(loss=nn_loss_choice_b,optimizer='adam')\n    \nprint(model)\nmodel.summary()\nkeras.utils.plot_model(model, \"model_nn1b.png\", show_shapes=True,show_layer_activations=False)\nmodel_list.append(model)\nlist_X.append(local_X)\nlist_y.append(local_y)\n\n\nmodel_labels.append(\"NN_1c_Dew_Auto\")\nmodel_titles.append(\"Neural Network 1c Dew Auto\")\n\nlocal_X=dengue_features_sj_train_lags[features_for_model].dropna()\nlocal_y=y_sj_multistep\n\nprint(local_X.shape[1])\ninputs = Input(shape=(local_X.shape[1],),name='input')\nprint(inputs)\n\nmodel = BuildMyNN((local_X.shape[1],),target_forecast_steps,\"NN_1c_Dew_Auto\")\nmodel.compile(loss=nn_loss_choice_c,optimizer='adam')\n    \nprint(model)\nmodel.summary()\nkeras.utils.plot_model(model, \"model_nn1c.png\", show_shapes=True,show_layer_activations=False)\nmodel_list.append(model)\nlist_X.append(local_X)\nlist_y.append(local_y)\n\n\n\n\n# Now with weighted training\nmodel_labels.append(\"NN_1d_Dew_Auto\")\nmodel_titles.append(\"Neural Network 1d Dew Auto\")\n\nlocal_X=dengue_features_sj_train_lags[features_for_model].dropna()\nlocal_y=y_sj_multistep\n\ninputs = Input(shape=(local_X.shape[1],),name='input')\n\nmodel = BuildMyNN((local_X.shape[1],),target_forecast_steps,\"NN_1d_Dew_Auto\")\nmodel.compile(loss=nn_loss_choice_d,optimizer='adam')\n    \nprint(model)\nmodel.summary()\nkeras.utils.plot_model(model, \"model_nn1d.png\", show_shapes=True,show_layer_activations=False)\nmodel_list.append(model)\nlist_X.append(local_X)\nlist_y.append(local_y)\n\n\n\n\n\n\n# ====================================================================\n\n\nmodel_labels.append(\"NN_2_Dew_Deriv\")\nmodel_titles.append(\"Neural Network 2 Dew Deriv\")\nfeatures_for_model=['y_lag_1','y_lag_2','y_lag_3','y_lag_4','y_lag_5',\n                    'y_lag_6','y_lag_7','y_lag_8','y_lag_9','y_lag_10',\n                    'target_deriv_smooth_const_mean_lag_1',\n                    'target_deriv_smooth_const_mean_lag_2',\n                    'target_deriv_smooth_const_mean_lag_3',\n                    'target_deriv_smooth_const_mean_lag_4',\n                    'target_deriv_smooth_const_mean_lag_5',\n                    'dew_pt_lag_1','dew_pt_lag_2','dew_pt_lag_3',\n                    'dew_pt_lag_4','dew_pt_lag_5','dew_pt_lag_6',\n                    'dew_pt_lag_7','dew_pt_lag_8','dew_pt_lag_9',\n                    'dew_pt_lag_10','dew_pt_lag_11','dew_pt_lag_12'\n                   ]\n\nlocal_X=dengue_features_sj_train_lags[features_for_model].dropna()\nlocal_y=y_sj_multistep\n\ninputs = Input(shape=(local_X.shape[1],),name='input')\n \nmodel = BuildMyNN((local_X.shape[1],),target_forecast_steps,\"NN_2_Dew_Deriv_Auto\")\nmodel.compile(loss=nn_loss_choice,optimizer='adam')\n\nprint(model)\nmodel.summary()\nkeras.utils.plot_model(model, f'model_nn_{model_labels[-1]}.png', show_shapes=True,show_layer_activations=False)\nmodel_list.append(model)\nlist_X.append(local_X)\nlist_y.append(local_y)\n\n\n# ====================================================================\n\nmodel_labels.append(\"NN_3a_Dew_Precip\")\nmodel_titles.append(\"Neural Network 3a Dew Precip\")\nfeatures_for_model=['dew_pt_lag_1','dew_pt_lag_2','dew_pt_lag_3',\n                    'dew_pt_lag_4','dew_pt_lag_5','dew_pt_lag_6',\n                    'dew_pt_lag_7','dew_pt_lag_8','dew_pt_lag_9',\n                    'dew_pt_lag_10','dew_pt_lag_11','dew_pt_lag_12',\n                    'precip_lag_1','precip_lag_2','precip_lag_3',\n                    'precip_lag_4','precip_lag_5','precip_lag_6',\n                    'precip_lag_7','precip_lag_8','precip_lag_9',\n                    'precip_lag_10','precip_lag_11','precip_lag_12'\n                   ]\n# adding in all y_lag \nfor col in dengue_features_sj_train_lags.columns:\n    if 'y_lag' in col:\n        features_for_model.append(col)\n\nlocal_X=dengue_features_sj_train_lags[features_for_model].dropna()\nlocal_y=y_sj_multistep\n\ninputs = Input(shape=(local_X.shape[1],),name='input')\n\nmodel = BuildMyNN((local_X.shape[1],),target_forecast_steps,\"NN_3a_Dew_Precip_Deriv_Auto\")\nmodel.compile(loss=nn_loss_choice_a,optimizer='adam')\n    \nprint(model)\nmodel.summary()\nkeras.utils.plot_model(model, \"model_nn3a.png\", show_shapes=True,show_layer_activations=False)\nmodel_list.append(model)\nlist_X.append(local_X)\nlist_y.append(local_y)\n\n\n\nmodel_labels.append(\"NN_3b_Dew_Precip\")\nmodel_titles.append(\"Neural Network 3b Dew Precip\")\nfeatures_for_model=['dew_pt_lag_1','dew_pt_lag_2','dew_pt_lag_3',\n                    'dew_pt_lag_4','dew_pt_lag_5','dew_pt_lag_6',\n                    'dew_pt_lag_7','dew_pt_lag_8','dew_pt_lag_9',\n                    'dew_pt_lag_10','dew_pt_lag_11','dew_pt_lag_12',\n                    'precip_lag_1','precip_lag_2','precip_lag_3',\n                    'precip_lag_4','precip_lag_5','precip_lag_6',\n                    'precip_lag_7','precip_lag_8','precip_lag_9',\n                    'precip_lag_10','precip_lag_11','precip_lag_12'\n                   ]\n# adding in all y_lag \nfor col in dengue_features_sj_train_lags.columns:\n    if 'y_lag' in col:\n        features_for_model.append(col)\n\nlocal_X=dengue_features_sj_train_lags[features_for_model].dropna()\nlocal_y=y_sj_multistep\n\ninputs = Input(shape=(local_X.shape[1],),name='input')\n\nmodel = BuildMyNN((local_X.shape[1],),target_forecast_steps,\"NN_3b_Dew_Precip_Deriv_Auto\")\nmodel.compile(loss=nn_loss_choice_b,optimizer='adam')\n    \nprint(model)\nmodel.summary()\nkeras.utils.plot_model(model, \"model_nn3b.png\", show_shapes=True,show_layer_activations=False)\nmodel_list.append(model)\nlist_X.append(local_X)\nlist_y.append(local_y)\n\n\n\n\nmodel_labels.append(\"NN_3c_Dew_Precip\")\nmodel_titles.append(\"Neural Network 3c Dew Precip\")\nfeatures_for_model=['dew_pt_lag_1','dew_pt_lag_2','dew_pt_lag_3',\n                    'dew_pt_lag_4','dew_pt_lag_5','dew_pt_lag_6',\n                    'dew_pt_lag_7','dew_pt_lag_8','dew_pt_lag_9',\n                    'dew_pt_lag_10','dew_pt_lag_11','dew_pt_lag_12',\n                    'precip_lag_1','precip_lag_2','precip_lag_3',\n                    'precip_lag_4','precip_lag_5','precip_lag_6',\n                    'precip_lag_7','precip_lag_8','precip_lag_9',\n                    'precip_lag_10','precip_lag_11','precip_lag_12'\n                   ]\n# adding in all y_lag \nfor col in dengue_features_sj_train_lags.columns:\n    if 'y_lag' in col:\n        features_for_model.append(col)\n\nlocal_X=dengue_features_sj_train_lags[features_for_model].dropna()\nlocal_y=y_sj_multistep\n\ninputs = Input(shape=(local_X.shape[1],),name='input')\n\nmodel = BuildMyNN((local_X.shape[1],),target_forecast_steps,\"NN_3c_Dew_Precip_Deriv_Auto\")\nmodel.compile(loss=nn_loss_choice_c,optimizer='adam')\n    \nprint(model)\nmodel.summary()\nkeras.utils.plot_model(model, \"model_nn3c.png\", show_shapes=True,show_layer_activations=False)\nmodel_list.append(model)\nlist_X.append(local_X)\nlist_y.append(local_y)\n\n\nmodel_labels.append(\"NN_3d_Dew_Precip\")\nmodel_titles.append(\"Neural Network 3d Dew Precip\")\nfeatures_for_model=['dew_pt_lag_1','dew_pt_lag_2','dew_pt_lag_3',\n                    'dew_pt_lag_4','dew_pt_lag_5','dew_pt_lag_6',\n                    'dew_pt_lag_7','dew_pt_lag_8','dew_pt_lag_9',\n                    'dew_pt_lag_10','dew_pt_lag_11','dew_pt_lag_12',\n                    'precip_lag_1','precip_lag_2','precip_lag_3',\n                    'precip_lag_4','precip_lag_5','precip_lag_6',\n                    'precip_lag_7','precip_lag_8','precip_lag_9',\n                    'precip_lag_10','precip_lag_11','precip_lag_12'\n                   ]\n# adding in all y_lag \nfor col in dengue_features_sj_train_lags.columns:\n    if 'y_lag' in col:\n        features_for_model.append(col)\n\nlocal_X=dengue_features_sj_train_lags[features_for_model].dropna()\nlocal_y=y_sj_multistep\n\ninputs = Input(shape=(local_X.shape[1],),name='input')\n\nmodel = BuildMyNN((local_X.shape[1],),target_forecast_steps,\"NN_3d_Dew_Precip_Deriv_Auto\")\nmodel.compile(loss=nn_loss_choice_d,optimizer='adam')\n    \nprint(model)\nmodel.summary()\nkeras.utils.plot_model(model, \"model_nn3a.png\", show_shapes=True,show_layer_activations=False)\nmodel_list.append(model)\nlist_X.append(local_X)\nlist_y.append(local_y)\n\n\n\n\n\n\n\n#                    'rel_hum_lag_6','rel_hum_lag_7','rel_hum_lag_8',\n#                    'rel_hum_lag_9','rel_hum_lag_10','rel_hum_lag_11',\n\n# ====================================================================\n\nmodel_labels.append(\"NN_4_Dew_Precip_Hum\")\nmodel_titles.append(\"Neural Network 4 Dew Precip Humid\")\nfeatures_for_model=['y_lag_1','y_lag_2','y_lag_3','y_lag_4','y_lag_5',\n                    'y_lag_6','y_lag_7','y_lag_8','y_lag_9','y_lag_10',\n                    'dew_pt_lag_1','dew_pt_lag_2','dew_pt_lag_3',\n                    'dew_pt_lag_4','dew_pt_lag_5','dew_pt_lag_6',\n                    'dew_pt_lag_7','dew_pt_lag_8','dew_pt_lag_9',\n                    'dew_pt_lag_10','dew_pt_lag_11','dew_pt_lag_12',\n                    'precip_lag_1','precip_lag_2','precip_lag_3',\n                    'precip_lag_4','precip_lag_5','precip_lag_6',\n                    'precip_lag_7','precip_lag_8','precip_lag_9',\n                    'precip_lag_10','precip_lag_11','precip_lag_12',\n                    'rel_hum_lag_1','rel_hum_lag_2','rel_hum_lag_3',\n                    'rel_hum_lag_4','rel_hum_lag_5','rel_hum_lag_6',\n                    'rel_hum_lag_7','rel_hum_lag_8','rel_hum_lag_9',\n                    'rel_hum_lag_10','rel_hum_lag_11','rel_hum_lag_12'\n                   ]\n\nlocal_X=dengue_features_sj_train_lags[features_for_model].dropna()\nlocal_y=y_sj_multistep\n\ninputs = Input(shape=(local_X.shape[1],),name='input')\n\nmodel = BuildMyNN((local_X.shape[1],),target_forecast_steps,\"NN_4_Dew_Precip_Hum_Deriv_Auto\")\nmodel.compile(loss=nn_loss_choice,optimizer='adam')\n    \nprint(model)\nmodel.summary()\nkeras.utils.plot_model(model, \"model_nn4.png\", show_shapes=True,show_layer_activations=False)\nmodel_list.append(model)\nlist_X.append(local_X)\nlist_y.append(local_y)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T02:16:30.198293Z","iopub.execute_input":"2023-09-13T02:16:30.198748Z","iopub.status.idle":"2023-09-13T02:16:35.713835Z","shell.execute_reply.started":"2023-09-13T02:16:30.198709Z","shell.execute_reply":"2023-09-13T02:16:35.712292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Split the data\nAlso, we can choose to standardize the data here (so we can make sure to just get the mean and std from the training sets)","metadata":{}},{"cell_type":"code","source":"# Switch to standardize all inputs\nenable_standardize_all=True\n\ntest_size=400\nfor i in range(len(list_X)):\n    local_X = list_X[i]\n    local_y = list_y[i]\n    local_y, local_X = local_y.align(local_X, join='inner', axis=0)\n    X_train, X_valid, y_train, y_valid = train_test_split(local_X, local_y, test_size=test_size, shuffle=False)\n\n    if (enable_standardize_all):\n        X_valid = (X_valid - X_train.mean())/X_train.std()\n        X_train = (X_train - X_train.mean())/X_train.std()\n    \n    X_train_list.append(X_train)\n    X_valid_list.append(X_valid)\n    y_train_list.append(y_train)\n    y_valid_list.append(y_valid)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T02:16:56.336689Z","iopub.execute_input":"2023-09-13T02:16:56.338060Z","iopub.status.idle":"2023-09-13T02:16:56.466897Z","shell.execute_reply.started":"2023-09-13T02:16:56.337992Z","shell.execute_reply":"2023-09-13T02:16:56.465742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Run the Fits","metadata":{}},{"cell_type":"code","source":"enable_early_stopping = False","metadata":{"execution":{"iopub.status.busy":"2023-09-13T02:16:59.568529Z","iopub.execute_input":"2023-09-13T02:16:59.568921Z","iopub.status.idle":"2023-09-13T02:16:59.575312Z","shell.execute_reply.started":"2023-09-13T02:16:59.568884Z","shell.execute_reply":"2023-09-13T02:16:59.574049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For fitting methods that give histories\nfit_histories=[]\nfit_history_labels=[]\nfit_history_indices=[]\n\n# Settings\nverbose_nn = 0\n# FIXME increase epochs after debugging\nnum_epochs=400\n#num_epochs=800\nbatch_size=10\n#batch_size=50\n\nes_patience = 20\n\n# Do I have to build an EarlyStopping object for each model?\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=es_patience)\n# history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=4000, verbose=0, callbacks=[es])\n\n\n\nfor i in range(len(model_list)):\n    model=model_list[i]\n    model_label=model_labels[i]\n    \n    X_train=X_train_list[i]\n    y_train=y_train_list[i]\n    \n    X_valid=X_valid_list[i]\n    y_valid=y_valid_list[i]\n    # can add choices based on tags in the model_label\n    # maybe add NN to the beginning\n    print(f'Fitting model {model_label}')\n    #print(y_train.head())\n  \n    if model_label[0:2] == \"NN\":\n        print(\"I think this is a Neural Net model. Fitting accordingly.\")\n\n        if enable_early_stopping:\n            history = model.fit(X_train,y_train,epochs=num_epochs,validation_data=(X_valid, y_valid),batch_size=batch_size,verbose=verbose_nn,callbacks=[es])\n        else:\n            history = model.fit(X_train,y_train,epochs=num_epochs,validation_data=(X_valid, y_valid),batch_size=batch_size,verbose=verbose_nn)\n            \n        #history = model.fit(X_train,y_train,epochs=num_epochs,batch_size=52,verbose=0)\n        #history = model.fit(X_train,y_train)\n        fit_histories.append(history)\n        fit_history_labels.append(model_label)\n        fit_history_indices.append(i)\n    else:\n    ##history = model.fit(X, {'cont_out': Y, 'cat_out': Z}, epochs=10, batch_size=8)\n    \n        model.fit(X_train,y_train)\n    #history = model.fit(X_train,y_train)\n    #fit_histories.append(history)\n    #fit_history_labels.append(model_label)\n    #fit_history_indices.append(i)\n    #print(type(history))\n    print(f'  Finished fitting model {model_label}')\n#history = model.fit(\n#    X_train, y_train,\n#    validation_data=(X_valid, y_valid),\n#    batch_size=256,\n#    epochs=100,\n#    verbose=0,\n#)\n\n\n# Show the learning curves\n#history_df = pd.DataFrame(history.history)\n#history_df.loc[:, ['loss', 'val_loss']].plot();\n","metadata":{"execution":{"iopub.status.busy":"2023-09-13T02:18:15.296412Z","iopub.execute_input":"2023-09-13T02:18:15.297470Z","iopub.status.idle":"2023-09-13T02:39:09.045714Z","shell.execute_reply.started":"2023-09-13T02:18:15.297423Z","shell.execute_reply":"2023-09-13T02:39:09.044254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfor i in range(len(fit_history_indices)):\n    history = fit_histories[i]\n    label=fit_history_labels[i]\n    model_label = model_labels[i]\n    fit_index=fit_history_indices[i] # global model index\n    print(f'Giving fitting history information for {label}')\n    print(type(history))\n    print(history)\n    history_df = pd.DataFrame(history.history)\n    print(history_df.columns)\n    # Start the plot at epoch 5\n    history_df.loc[5:, ['loss', 'val_loss']].plot()\n    plt.legend([f'Model {model_label} Training Loss',f'Model {model_label} Validation Loss'])\n    plt.savefig(f'model_{label}_fit_history.png')\n    #history_df.loc[5:, ['y_step_10_loss', 'val_y_step_10_loss']].plot()\n    #plt.legend(['Training Loss (Step 10)','Validation Loss (Step 10)'])\n    #plt.savefig(f'model_{label}_fit_history.png')","metadata":{"execution":{"iopub.status.busy":"2023-09-13T02:48:04.418529Z","iopub.execute_input":"2023-09-13T02:48:04.419217Z","iopub.status.idle":"2023-09-13T02:48:08.395706Z","shell.execute_reply.started":"2023-09-13T02:48:04.419160Z","shell.execute_reply":"2023-09-13T02:48:08.394488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Produce Predictions","metadata":{}},{"cell_type":"code","source":"y_pred_train_list=[]\ny_pred_valid_list=[]\n\nfor i in range(len(model_list)):\n    model=model_list[i]\n    model_label=model_labels[i]\n    X_train=X_train_list[i]\n    y_train=y_train_list[i]\n    X_valid=X_valid_list[i]\n    y_valid=y_valid_list[i]\n    \n    print(f'Producing predictions for model {model_label}')\n    if model_label[0:2] == \"NN\":\n        print(\"I think this is a Neural Net model. Formatting input and output accordingly.\")\n\n        temp_predict_train = model.predict(X_train)\n        print(type(temp_predict_train[0]))\n\n        y_pred_train = pd.DataFrame(temp_predict_train,index=y_train.index,columns=y_train.columns)\n\n        print(f'y_pred_train df has shape {y_pred_train.shape}')\n        \n        temp_predict_valid = model.predict(X_valid)\n        print(type(temp_predict_valid))\n        #if model_label != 'NN_1_Dew':\n        #    temp_predict_valid=np.concatenate(temp_predict_valid,axis=1)\n        \n        y_pred_valid = pd.DataFrame(temp_predict_valid,index=y_valid.index,columns=y_valid.columns)\n    \n    else:\n        y_pred_train = pd.DataFrame(model.predict(X_train),index=y_train.index,\n                              columns=y_train.columns,).clip(0.0)\n        y_pred_valid = pd.DataFrame(model.predict(X_valid),index=y_valid.index,\n                              columns=y_valid.columns,).clip(0.0)\n    \n    y_pred_train_list.append(y_pred_train)\n    y_pred_valid_list.append(y_pred_valid)\n    ","metadata":{"execution":{"iopub.status.busy":"2023-09-13T02:48:25.538854Z","iopub.execute_input":"2023-09-13T02:48:25.539504Z","iopub.status.idle":"2023-09-13T02:48:29.207467Z","shell.execute_reply.started":"2023-09-13T02:48:25.539460Z","shell.execute_reply":"2023-09-13T02:48:29.206400Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Run Metrics","metadata":{}},{"cell_type":"code","source":"import tensorflow_addons as tfa","metadata":{"execution":{"iopub.status.busy":"2023-09-13T02:48:35.872630Z","iopub.execute_input":"2023-09-13T02:48:35.873047Z","iopub.status.idle":"2023-09-13T02:48:36.005993Z","shell.execute_reply.started":"2023-09-13T02:48:35.872989Z","shell.execute_reply":"2023-09-13T02:48:36.004931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rmse_train_list=[]\nrmse_valid_list=[]\nrsquare_list=[]\n\n# metric for NNs\nr_square_metric = tfa.metrics.RSquare()\n\nfor i in range(len(model_list)):\n    model=model_list[i]\n    model_label=model_labels[i]\n    \n    X_valid=X_valid_list[i]\n    y_train=y_train_list[i]\n    y_valid=y_valid_list[i]\n    y_pred_train=y_pred_train_list[i]\n    y_pred_valid=y_pred_valid_list[i]\n\n    print(f'y_train has type {type(y_train)}')\n    print(f'y_pred_train has type {type(y_pred_train)}')\n    \n    print(f'y_train has shape {y_train.shape}')\n    print(f'y_pred_train has shape {y_pred_train.shape}')\n\n    train_rmse = mean_squared_error(y_train, y_pred_train, squared=False)\n    test_rmse = mean_squared_error(y_valid, y_pred_valid, squared=False)\n    \n    if model_label[0:2] == \"NN\":\n        print(type(model))\n        # storing 3, an impossible value, for now\n        r_square=3\n        #r_square = tfa.metrics.RSquare\n        #r_square=model.score(X_valid,y_valid)\n    else:\n        r_square=model.score(X_valid,y_valid)\n    \n    rmse_train_list.append(train_rmse)\n    rmse_valid_list.append(test_rmse)\n    rsquare_list.append(r_square)\n    print('===========================================')\n    print(f'Model {model_label}')\n    print(f' RMSE(training,valid)=({train_rmse:.2f},{test_rmse:.2f})')\n    print(f'           R^2(valid)=({r_square:.2f})')\n#print('Model Two:')\n#print((f\"Train RMSE: {train_rmse:.2f}\\n\" f\"Test RMSE: {test_rmse:.2f}\"))\n#print('')\n\n#r_square=model_2.score(X_2_valid,y_2_valid)\n#print(f\"Model 2 Train R^2: {r_square:.2f}\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-09-13T02:48:38.373917Z","iopub.execute_input":"2023-09-13T02:48:38.375231Z","iopub.status.idle":"2023-09-13T02:48:38.822180Z","shell.execute_reply.started":"2023-09-13T02:48:38.375183Z","shell.execute_reply":"2023-09-13T02:48:38.821166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(rmse_train_list)\nprint(rmse_valid_list)\nprint(rsquare_list)\nfig, ax = plt.subplots()\nplt.plot(model_labels,rmse_valid_list,marker='o',linestyle='')\nplt.xticks(rotation = 90,fontsize=7)\nax.set_ylabel('Validation RMSE')\nplt.savefig('rmse_comparison.png')","metadata":{"execution":{"iopub.status.busy":"2023-09-13T02:51:04.214986Z","iopub.execute_input":"2023-09-13T02:51:04.215813Z","iopub.status.idle":"2023-09-13T02:51:04.606336Z","shell.execute_reply.started":"2023-09-13T02:51:04.215771Z","shell.execute_reply":"2023-09-13T02:51:04.605229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plot Results","metadata":{}},{"cell_type":"code","source":"\nfor i in range(len(model_list)):\n    y_pred_train = y_pred_train_list[i]\n    y_pred_valid = y_pred_valid_list[i]\n\n    model_name  = model_labels[i]\n    model_title = model_titles[i]\n\n    EVERY = 20\n\n    START=50\n    END=550\n    \n    fig, (ax1,ax2) = plt.subplots(2, 1, figsize=(15, 10))\n    line1 = y_sj.loc[START:END].plot(ax=ax1,label='Data',color='black')\n\n    x=plot_multistep(y_pred_train.loc[START:END],ax=ax1,every=EVERY)\n    ax1.set_ylabel('Weekly Cases')\n    ax1.legend(['Data (Training)',f'Model {i+1}:{model_title} Forecasts (training)'])\n\n    EVERY = 20\n\n    START=600\n    END=1100\n\n    #fig, ax = plt.subplots(1, 1, figsize=(11, 4))\n    line1 = y_sj.loc[START:END].plot(ax=ax2,label='Data',color='black')\n\n    x=plot_multistep(y_pred_valid.loc[START:END],ax=ax2,every=EVERY)\n\n    ax2.set_ylabel('Weekly Cases')\n    ax2.legend(['Data (Validation)',f'Model {i+1}:{model_title} Forecasts'])\n\n    plt.savefig(f'model_{i+1}_{model_name}.png')","metadata":{"execution":{"iopub.status.busy":"2023-09-13T02:54:13.172493Z","iopub.execute_input":"2023-09-13T02:54:13.173702Z","iopub.status.idle":"2023-09-13T02:54:28.484295Z","shell.execute_reply.started":"2023-09-13T02:54:13.173653Z","shell.execute_reply":"2023-09-13T02:54:28.483035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# New: try to plot the forecast for n weeks into the future\n\nnth_step=5\n\nfor i in range(len(model_list)):\n    y_pred_train = y_pred_train_list[i]\n    y_pred_valid = y_pred_valid_list[i]\n\n    model_name  = model_labels[i]\n    model_title = model_titles[i]\n\n    EVERY = 20\n\n    START=50\n    END=550\n    \n    fig, (ax1,ax2) = plt.subplots(2, 1, figsize=(15, 10))\n    line1 = y_sj.loc[START:END].plot(ax=ax1,label='Data',color='black')\n\n    #x=plot_multistep(y_pred_train.loc[START:END],ax=ax1,every=EVERY)\n    x = plot_nth_step(y_pred_train.loc[START:END],nstep=nth_step,ax=ax1)\n    ax1.set_ylabel('Weekly Cases')\n    ax1.legend(['Data (Training)',f'Model {i+1}:{model_title} Forecasts (training)'])\n\n    EVERY = 20\n\n    START=600\n    END=1100\n\n    #fig, ax = plt.subplots(1, 1, figsize=(11, 4))\n    line1 = y_sj.loc[START:END].plot(ax=ax2,label='Data',color='black')\n\n    x = plot_nth_step(y_pred_valid.loc[START:END],nstep=nth_step,ax=ax2)\n    # def plot_nth_step(y, nstep=1,ax=None,palette_kwargs=None):\n    #x=plot_multistep(y_pred_valid.loc[START:END],ax=ax2,every=EVERY)\n\n    ax2.set_ylabel('Weekly Cases')\n    ax2.legend(['Data (Validation)',f'Model {i+1}:{model_title} {nth_step}-week Forecasts'])\n\n    plt.savefig(f'nth_step_model_{i+1}_{model_name}.png')","metadata":{"execution":{"iopub.status.busy":"2023-06-25T23:33:45.259889Z","iopub.execute_input":"2023-06-25T23:33:45.260372Z","iopub.status.idle":"2023-06-25T23:33:56.112233Z","shell.execute_reply.started":"2023-06-25T23:33:45.260328Z","shell.execute_reply":"2023-06-25T23:33:56.111079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Thoughts\n\n* These models don't explicitly include the time of year\n    * However common ways of including the time of year are to put in fourier terms (sin(n*t),cos(n*t)). But the weather data (especially the temperature) are basically already sin(x/52),cos(x/52) features.\n    * ","metadata":{}},{"cell_type":"markdown","source":"\nWill also try some following ideas from https://www.tensorflow.org/tutorials/structured_data/time_series:\n* Long Term Short Term RNNs\n    * This might be designed for a different input format, without the explicitly calculated lags. But in principle, one could try it. There could even by a mix\n* CNNs (read that this can work with time series)\n* Residual Wrapper, which uses the output of the network for the change in cases, not the absolute value\n\n","metadata":{}}],"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}}